# -*- coding: utf-8 -*-
"""BreastCancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PWjfm8zjR8gbMuXqB07K-9Zdxj9WyWnq

# **Importacion de las librerias**
"""

from google.colab import drive
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Importacion de la base de datos**"""

drive.mount("/content/gdrive")  # utiliza comando

!pwd

# Commented out IPython magic to ensure Python compatibility.
#put your own path in google drive
# %cd "/content/gdrive/MyDrive/Universidad/7mo Semestre/Inteligencia artificial avanzada"
!ls

"""Importamos el dataset, y hacemos las acciones necesarias para limpiarlo(eliminamos la columna 'id' y cambiamos los valores del diagnostico por 0 y 1).

# **Analisis exploratorio**
"""

df = pd.read_csv("breast-cancer.csv")
df = df.drop(["id"], axis = 1)
df.diagnosis = df.diagnosis.map({"M":1, "B":0})
df.shape

"""Realizamos un heatmap para ver que variables estan influyen mas, ya sea positiva o negativamente sobre nuestra variable dependiente, que en este caso sera 'diagnosis'."""

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

"""Eliminamos las columnas que tienen minima relacion o relacion nula con nuestra variable a predecir."""

df = df.drop(["fractal_dimension_se","symmetry_se","smoothness_se","texture_se", "fractal_dimension_mean"], axis = 1)
df.head(1)

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

"""Dividimos el dataframe a mano con ayuda de la libreria numpy, se divide de manera aleatoria, 70% del dataframe sera para training y 30% para testing.

# **Tratamiento de datos para el algoritmo**
"""

p_train = 0.70 # Porcentaje de train.

df['is_train'] = np.random.uniform(0, 1, len(df)) <= p_train
train, test = df[df['is_train']==True], df[df['is_train']==False]
df = df.drop('is_train', 1)

print("Ejemplos usados para entrenar: ", len(train))
print("Ejemplos usados para test: ", len(test))

"""Una vez realizado la division eliminamos laa columnaa de soporte('is_train') para realizar esta division"""

train = train.drop(["is_train"], axis = 1)
test = test.drop(["is_train"], axis = 1)

train.head(1)

"""Declaramos nuestras variables dependientes y la variable independiente tanto en el dataframe de training como en el de testing."""

x_train = train.drop(["diagnosis"], axis = 1)
y_train = train[["diagnosis"]]

x_test = test.drop(["diagnosis"], axis = 1)
y_test = test[["diagnosis"]]

# Datos de entrenamiento
x_train = x_train.values
y_train = y_train.values

# Datos para testing
x_test = x_test.values
y_test = y_test.values

x_train.T

"""Verificamos que tengan la misma cantidad de columnas y vemos sus dimensiones."""

print("Dimensiones del training")
print("Dimension de x: ", x_train.shape)
print("Dimension de y: ", y_train.shape)

print("Dimensiones del testing")
print("Dimension de x: ", x_test.shape)
print("Dimension de y: ", y_test.shape)

"""Haremos traspuestas nuestras columnas para asi poder implementarlas en la regresion logistica."""

x_train = x_train.T
y_train = y_train.reshape(1, x_train.shape[1])

x_test = x_test.T
y_test = y_test.reshape(1, x_test.shape[1])

print("Dimensiones del training despues de la transpuesta")
print("Dimension de x", x_train.shape)
print("Dimension de y", y_train.shape)

print("Dimensiones del testing despues de la transpuesta")
print("Dimension de x", x_test.shape)
print("Dimension de y", y_test.shape)

"""# **Algoritmo**

Definimos la funcion de la sigmoide
"""

def sigmoid(x):
  s = 1/(1+np.exp(-(x)))
  return s

print(x_train.shape[1])
print(x_train.shape[0])

def model(x_train, y_train, alpha, iterations):
  m = x_train.shape[1] # Registros → Valores de las variables
  n = x_train.shape[0] # Columnas → Variables

  w = np.zeros([n,1])

  lista_costo = []
  """
  Creamos un vector con tamaño igual al numero de variables
  estos seran los pesos iniciales de nuestras variables.
  """
  b = 0

  for i in range(iterations):
    sigma = np.dot(w.T, x_train) + b # Prediccion probabilistica
    y_pred = sigmoid(sigma)

    # Funcion de costo
    costo = -(1/m)*np.sum( y_train*np.log(y_pred) + (1-y_train)* np.log(1-y_pred) )

    # Gradiente descendiente
    dw = (1/m)*np.dot(y_pred-y_train, x_train.T)
    db = (1/m)*np.sum(y_pred-y_train)

    w = w - alpha*dw.T
    b = b - alpha*db

    # Descenso de nuestra funcion de costo
    lista_costo.append(costo)

    if(i%(iterations/10) == 0): # Comentar
      print("En la iteracion numero", i, "el error es de : ", costo)
    
  return w,b,lista_costo

iterations = 100000
alpha = 0.00000000015 # Cantidad de ceros: 9
w, b, lista_costo = model(x_train, y_train, alpha, iterations)

"""Graficamos la disminucion del error segun el numero de iteraciones para ver en que momento existe un estancamiento del algoritmo."""

plt.plot(np.arange(iterations), lista_costo)
plt.show()

"""# **Precision**

Ahora veremos la precision del modelo:
"""

def accuracy(x_test, y_test, w, b):
  sigma = np.dot(w.T, x_test) + b # Prediccion probabilistica
  y_pred = sigmoid(sigma)

  y_pred = y_pred > 0.5
  y_pred = np.array(y_pred, dtype = 'int64')
  """
  Con las dos lineas de codigo anteriores convertimos a booleanos
  y luego a un formato binario(0,1), esto recordando que con la
  funcion sigmoide los valores menores a 0.5 se tomaran como 0 
  y los mayores a 0.5 como 1
  """
  acc = (1 - np.sum(np.absolute(y_pred - y_test))/y_test.shape[1])*100
  print("La precision del modelo es: ", round(acc, 2), "%")

"""Calculamos la precision en el dataset de entrenamiento:"""

accuracy(x_train, y_train, w, b)

"""Calculamos la precision en el dataset de validacion:"""

accuracy(x_test, y_test, w, b)

"""# **Segunda prueba del algoritmo con diferentes valores**"""

iterations = 100000
alpha = 0.0015 # Cantidad de ceros: 2
w, b, lista_costo = model(x_train, y_train, alpha, iterations)

plt.plot(np.arange(iterations), lista_costo)
plt.show()

accuracy(x_train, y_train, w, b)

accuracy(x_test, y_test, w, b)

"""# **Tercera prueba del algoritmo con diferentes valores**"""

iterations = 100000
alpha = 0.00000015 # Cantidad de ceros: 6
w, b, lista_costo = model(x_train, y_train, alpha, iterations)

plt.plot(np.arange(iterations), lista_costo)
plt.show()

accuracy(x_train, y_train, w, b)

accuracy(x_test, y_test, w, b)

"""# **Cuarta prueba del algoritmo con diferentes valores:**"""

iterations = 100000
alpha = 0.000000015 # Cantidad de ceros: 7
w, b, lista_costo = model(x_train, y_train, alpha, iterations)

plt.plot(np.arange(iterations), lista_costo)
plt.show()

accuracy(x_train, y_train, w, b)

accuracy(x_test, y_test, w, b)